\documentclass[journal, a4paper]{IEEEtran}

\usepackage{graphicx}  

\usepackage{url}  

\usepackage{amsmath}    

\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{wrapfig}
\usepackage{xeCJK}
\setCJKmainfont[
  BoldFont=WenQuanYi Zen Hei,
  ItalicFont=AR PL KaitiM GB]
  {AR PL SungtiL GB}
\setCJKsansfont{WenQuanYi Zen Hei}
\setCJKmonofont{cwTeXFangSong}

\begin{document}

    \title{基于动态规划选取相机路线的视频防抖算法}
    \author{刘璐 13307130520}
    \maketitle

\section{引言}
    随着摄影录像技术的发展，越来越多的人选择使用小型手持设备进行视频录制，记录下一些难忘的时刻。大部分使用小型手持设备进行视频录制的使用者在录制过程中一方面缺乏固定镜头的设备，另一方面没有专业的录像技术，以致于使用小型手持设备录制的视频都会出现一定的画面抖动，在一定程度上影响了观影效果。视频防抖技术的出现即是为了解决这一难题。 \\
    
    要减少视频中影响视频质量的抖动现象，一种方法是使用手持设备的陀螺仪、加速度仪，记录视频拍摄时设备在三维空间三个角度的旋转以及加速度，从设备的移动路径反向恢复视频的质量。这样的方法能获取录制设备的移动轨迹，从而提高视频处理的效果\cite{GYR1}\cite{GYR2}。但是，一方面，对于网络上大部分已存在的视频来说，录制时的设备移动记录已经无法获取；另一方面，不同的设备内置的陀螺仪、加速度仪会有不同的误差，要设计对于所有设备都行之有效的算法有一定的难度。\\
    
    另一种方法是，直接对视频进行处理。传统的视频防抖方法都是基于2D图像进行处理的。一般的处理方法为对相邻的帧进行特征点匹配，从帧与帧之间的对齐关系获得录像设备视角的移动路径，再对视角的移动路线进行平滑处理，使得输出视频的移动相对稳定\cite{L1Opt}。也有新兴的技术通过视频重构三维模型，来实现视频防抖。该技术重构像素点的3D点云，以及录影设备在三维空间上的移动路线，对设备移动路线进行平滑处理后，利用3D点云恢复视频图像\cite{FLiu1}\cite{FLiu2}。 \\
    
    我的方法主要是基于2D图像进行处理的。在对相机移动线路进行平滑处理时，只考虑在$x,y,\alpha$三个分量上的线性移动，利用折线近似原始的相机移动路线。在拟合过程中，选定若干关键帧为不动帧，从一个关键帧到另一个关键帧的过程中，规划的相机路径始终采用相同的变换进行移动。在关键帧选取的关键环节，运用动态规划算法，最优化关键帧的选取，从而获得尽量与原始相机移动路径相似的线性移动路径，实现视频的防抖。

\section{视频防抖算法}
\subsection{固定视角的视频防抖}
    刚开始的时候，我采用了Matlab手册中基于特征点匹配的视频防抖方法\cite{Matlab}。首先，对于视频中的每一帧图像$I_{1}, I_{2}, \cdots, I_{n}$，求SURF特征点。然后，对视频中相邻两帧的图像$(I_{t}, I_{t+1})$进行特征点匹配，获得相邻两帧的变换矩阵$F_{t+1}$。相机路径$C_{t}$与前一帧的相机路径$C_{t-1}$以及变换矩阵$F_{t}$相关：$C_{t}=C_{t-1}*F_{t}$，且对$C_{t}$进行迭代计算可得到$C_{t}=F_{t}F_{t-1}\cdots F_{1}$。 \\
    
    进行视频防抖最直接的想法就是：对每一帧原始图像，利用$C_{t}$进行反变换，这样一来画面中的大部分特征点都会基本保持静止，从而实现了减少视频抖动的目的。这样的方法在固定视角、单一场景的视频中的确能取得较好的效果。但是，在一般的完整视频中，往往会有相机的移动、场景的切换。因为该方法的所有帧都是向第一帧对齐的，后面的帧中特征点的位置基本与第一帧中特征点的位置吻合，所以在遇到相机移动和场景切换的时候，相机的拍摄内容已经移出或部分移出第一帧的取景框，这个时候在第一帧的取景框内的内容即为缺省值。因此，该方法对存在场景切换的视频是基本无效的。
    
\subsection{选取固定关键帧的视频防抖}
    考虑到上述方法失效的原因主要是原始相机路径的移动与输出视频的取景框固定不动之间的矛盾。所以，我开始寻求能让取景框动起来的方法。 \\
    
    在这个过程中，我读到了M. Grundmann提出的利用L1范数约束优化相机路径的方法\cite{L1Opt}。在该论文中提到，在专业的视频拍摄中，相机的移动路线$P(t)$主要分为以下三种：
    \begin{itemize}
    \item 固定不动的相机路线，$DP(t)=0$
    \item 以恒定速度移动的相机路线，$DP^2(t)=0$
    \item 以恒定加速度移动的相机路线，$DP^3(t)=0$
    \end{itemize}
    在该论文中，M. Grundmann旨在用上述三种相机移动路径来近似原始的相机路径，使得输出的视频结果既保持了原来的视频内容，又实现了防抖的效果。在该论文中，M. Grundmann最终采用了L1范数约束线性规划的方法进行路径优化。\\
    
    固定不动的相机路线的视频防抖问题在上一小节中已被解决。我认为，长时间以恒定加速度移动的相机路线只在有专业的控制相机移动路线的设备上才有可能获得。一般在使用手持设备进行拍摄的条件下，使用者在移动相机时的目的在于获得恒定速度移动的相机路线，在这种情况下，恒定加速度移动的相机路线只会在使用者希望相机改变移动速度的短暂时刻出现。所以，我决定采用折线近似原始输入视频的相机路线。\\
    
   一开始，我采用了固定间隔的关键帧选取，如图\ref{fig:naive_motion}。每$k(k=30)$帧选一帧作为关键帧，关键帧的相机路径与路径规划后的相机路径$P_t$吻合，$P_t=C_t$，并假设取景框在关键帧$C_{ki}$与$C_{ki+k}$之间以恒定的速度移动，即变换矩阵相同，
   $$H_{ki+1}=H_{ki+2}=\cdots=H_{ki+k}\eqno{(1)}$$
   由于关键帧的性质，应有
   $$H_{ki+1}*H_{ki+2}*\cdots*H_{ki+k}=F_{ki+1}*F_{ki+2}*\cdots*F_{ki+k}\eqno{(2)}$$
   由等式($1$)($2$)得：$$H_{ki+j}=(F_{ki+1}*F_{ki+2}*\cdots*F_{ki+k})^{\frac{1}{k}}\eqno{(3)}$$从而可以求得每一帧图像的变换矩阵
   $$T_{ki+j}=H_{ki+j}^{-j}*F_{ki+1}*F_{ki+2}*\cdots*F_{ki+j}\eqno{(4)}$$
   这样一来，以固定间隔取关键帧，关键帧间的取景窗一直以相同的变换矩阵进行变换，就实现了在两个关键帧之间的画面以比较平滑的方式进行移动、变换。\\
   \begin{figure}[!hbt]
        \begin{center}
        \includegraphics[width=\columnwidth]{naive_motion.png}
        \caption{蓝色的折线是视频原始的相机在$x$方向的移动，橙色的折线是每$k(k=30)$帧取一帧作为关键帧，用关键帧之间的折线去近似相机移动的结果。可以看到，在第$30-60,150-270,330-360$帧的折线结果较符合真实情况，但是在$0-30,90-150,270-300$帧间，橙色折线不能很好的刻画相机的移动路线}
        \label{fig:naive_motion}
        \end{center}
    \end{figure}
  
   该方法较上一小节的方法而言，能够在一定程度上解决场景切换的问题，取景框不再是永远固定在某个特定的位置，而是可以随着相机的移动，以相对平滑的方式进行切换。但是，问题依然存在。以固定间隔取关键帧的方式依然不太合理。该算法存在着这样的假设：处于两个关键帧中间的帧在原始相机路径中以比较相似的方式进行变换。这个假设不对所有关键帧成立，所有关键帧间隔的选取就会对输出视频效果造成一定的影响。关键帧间隔取得太短，会使得视频依然存在明显的抖动现象；而关键帧间隔太长，很可能会出现相机移动的路径与假设的路径大相径庭。一个输出视频质量会被影响的例子是：后一个关键帧对应的相机位置在前一个关键帧的相机位置的正右方，且相机朝向一致角度一致。在该算法中，假设了在这两个关键帧之间，相机一直是向正右方移动的。但是，有可能相机的真实路径是先向正左方移动，再折回向正右方移动的。在这样的情况下，该算法规划的相机路径不能很好地刻画真实的相机路径，从而导致出现大量缺省画面的情况。
   
\subsection{基于动态规划的关键帧选取}
    为了解决上一小节中提到的问题，我最终采用了动态规划的方法来选取关键帧。要采用动态规划的方法解决该相机路径优化的问题，需要先对每一种关键点选取方案设计代价函数。 \\
    
    我设定动态规划的优化目标为j尽量最小化每一帧的相机真实位置与取景框的“偏离程度”。对于每一帧的“偏离程度”，我选择用水平方向位移$\Delta x$，垂直方向位移$\Delta y$，以及旋转的角度$\Delta\alpha$来刻画，具体“偏离程度”定义为：
    $$Diff(t)=\Delta x^2(t)+\Delta y^2(t)+w_\alpha\Delta\alpha^2(t)\eqno{(5)}$$
    $$\Delta x(t)=(T_t)_x-(F_{p+1}F_{p+2}\cdots F_{t})_x$$
    $$\Delta y(t)=(T_t)_y-(F_{p+1}F_{p+2}\cdots F_{t})_y$$
    $$\Delta x(t)=(T_t)_\alpha-(F_{p+1}F_{p+2}\cdots F_{t})_\alpha$$
    其中，$(A)_x,(A)_y,(A)_\alpha$分别表示取变换矩阵$A$的$x$分量，$y$分量以及$\alpha$角度分量，$p$为出现在第$t$帧之前的最后一个关键帧。考虑到角度$\Delta\alpha$的变化范围在$[-\pi,\pi]$之间，小的角度变化也可能造成较大的差异，所以角度分量在$Diff(t)$中应该乘以一个大于1的权值$(w_\alpha>1)$。我将变换矩阵都简化成了只包含旋转和位移变换的形式，则所有变换矩阵满足$$A = \begin{pmatrix}cos(\alpha) & -sin(\alpha) &0\\sin(\alpha)&cos(\alpha)&0\\dx&dy&1\end{pmatrix}$$
    即从变换矩阵可以方便的求出$x$分量，$y$分量以及$\alpha$角度分量。\\
    
    设计好每一帧的“偏离程度”计算函数后，整个关键帧选取方案的代价$Cost(1..n)$应为中间每一帧“偏离程度”的总和。但是，直接让所有帧的“偏离程度”总和作为评判整个选取方案的好坏的唯一指标且不加任何限制条件的话，结果必然是每一帧都被选择作为关键帧，这是使得“偏离程度”总和为零的最佳关键帧选取方案。但显然，这并不是我想要的结果。\\
    
    要使得整个输出视频较为平滑，则希望选择的关键点数量尽量越少越好，相邻关键点之间的间隔越大越好。换句话说，有的时候，我们可以牺牲一点“偏离程度”，来换取更平滑的视觉效果。最终，我将关键帧选取方案的代价$Cost(1..n)$设计为：
    $$Cost(1..n)=\sum\limits_{t=1}^n\frac{Diff(t)}{(q-p+1)^2}\eqno{(6)}$$
    其中，$p$为在$t$之前的最后一个关键帧，$q$为在$t$之后的第一个关键帧。以此来达到使得相邻的关键点之间的间隔尽量大的目的。\\
    
    另外，为了使得两个关键帧之间的距离不要太近，还需要设置相邻关键帧距离的下限$lb$，选取关键帧时要求每两个关键帧的距离必须大于$lb$。\\

    至此，对于确定的关键帧选取方案，整个方案的代价已经能被计算了。剩下的任务即为在所有的关键帧选取方案，选出其中代价最小的方案作为最终的关键帧选取方案。倘若暴力地枚举每一个关键帧的位置，枚举每一种可能的关键帧选取方案，时间上的开销是难以承受的。所以，这里需要运用动态规划的思想，将原问题分解为较小规模的子问题进行求解。\\
    
    设计动态规划函数$DP(i)$表示取第$i$帧作为关键帧的前提下，$Cost(1..t)$的大小。容易从等式$(6)$推广获得： $$Cost(i..j)=\sum\limits_{t=i}^j\frac{Diff(t)}{(q-p+1)^2}\eqno{(7)}$$
    所以，要求$DP(i)$的值，需要通过枚举确定上一个关键帧，由此得到动态规划转移方程：
    $$DP(i)=\min\limits_{1\leq j\leq i-lb}\left\{ DP(j)+Cost(i..j)\right\}\eqno{(8)}$$
    通过该状态转移方程，可以在$O(n^2T(Cost(i..j))$的时间内获得最佳的关键帧选取方案。在确定$i,j$为关键帧后，计算$Cost(i..j)$需要对$i,j$之间的每一帧$t$计算$Diff(t)$，计算单帧的$Diff(t)$可在常数时间内完成。所以整个动态规划算法的时间复杂度为$O(n^3)$。\\
    
\section{缺省画面处理算法}
    在完成对相机路径进行规划的工作以后，我发现为了使得视频的抖动减少，优化后的大部分画面与取景框不完全相同，导致了取景框内出现了缺省的像素点。在未经处理时，这些像素点灰度值都为0。在观影时，虽然画面内容的抖动变小了，但是画面的边框一直在抖动，对观影效果有一定程度的影响。所以，要使得视频防抖的输出视频效果较好，还需要对输出画面中的缺省值进行处理。我尝试了两种比较简单的方法。
\subsection{缩小取景框的方法}
    我尝试的第一个方法，也是非常简单的一个方法是缩小取景框的大小,如图\ref{fig:resize}。在规划的相机路径与真实相机路径高度相似的情况下，图像边缘缺省像素并不多，缩小取景框的处理方法是简单有效的。\\
    \begin{figure}[!hbt]
        \begin{center}
        \includegraphics[width=\columnwidth]{resize.png}
        \caption{缩小取景框的大小以减少画面边框的抖动。左图是经过路径规划后的输出，有“边缘黑框”情况，右图是经过缩小取景框操作后的输出}
        \label{fig:resize}
        \end{center}
    \end{figure}

    但是，这个方法也存在着两大问题。第一，当路径规划后的取景框与原始相机位置相差较大的时候，缺省像素点较多，这时候即使取景框有一定程度的缩小，但依然会存在缺省像素点的问题，所以这个问题并没有从根源得到解决。第二，被视频中裁剪掉的部分有可能有一些关键的信息，被裁剪后也会影响观影效果，如图\ref{fig:resize_fail}。
    \begin{figure}[!hbt]
        \begin{center}
        \includegraphics[width=\columnwidth]{resize_fail.png}
        \caption{左图为原始图像，右图为防抖后缩小取景框的效果。右图中小孩的头在画面中被裁剪掉了，这是一个图像中较为关键的部分，对这部分的裁剪使得整个画面的观影效果受到了影响。}
        \label{fig:resize_fail}
        \end{center}
    \end{figure}

\subsection{采用拼接相邻若干帧补全缺省部分的方法}
    裁剪掉画面边缘的做法显然不够理想。受到对多幅图片求全景图的方法的启发，我尝试用当前帧的前后$m$帧图像拼接成全景图后对当前帧缺省部分进行填充。
    这样的方法在缺省内容是视频的背景或固定不动的物体时，效果较好。图\ref{fig:padding_good}是补全缺省部分比较好的结果，在这一帧当中，需要补全的缺省部分比较少；同时，缺省部分基本上为背景的草丛、草地，基本不受移动物体（如图像中的人）的影响，所以使用相邻若干帧的全景图补全可以比较好地还原背景信息。\\
    \begin{figure}[!hbt]
        \begin{center}
        \includegraphics[width=\columnwidth]{padding_good.png}
        \caption{拼接相邻若干帧补全缺省信息的方法有效的例子。左图为经过防抖优化后的原始成功图像，右图为防抖后补全缺省部分的效果}
        \label{fig:padding_good}
        \end{center}
    \end{figure}
    
    但是当缺省内容包含移动的物体时，结果就不是那么令人满意了。由于在每一帧与当前帧进行对齐的时候，大部分有效的特征点都是背景点，所以移动物体在多幅图像中不能很好地对齐，这使得拼接的结果往往无法很好地还原移动物体的位置和形状。图\ref{fig:padding_fail}就是一个不理想的结果。可以看到，图像下方的缺省部分草坪，基本上得到了很好的还原。图像左边的缺省信息补全的结果则略显凌乱。仔细观察可以发现，对于不移动的物体（凳子）来说，也得到了比较好的还原。但是补全的图像中，出现了一个人，这个人在该帧的原始图像中是没有出现的，而且这个人在相邻的若干帧图像中，是处于移动状态的。所以在拼接的时候，出现了一个没有得到很好的拼接的人。这样的结果一方面，通过前后若干帧图像还原了一部分当前帧的缺省信息；但是另一方面，这样的缺省信息补全却是不完美的，这样别扭不自然的拼接在观影中会对观影者造成一定程度的干扰。这样的缺陷是由全景图拼接方法本身的局限性所带来的。\\
    \begin{figure}[!hbt]
        \begin{center}
        \includegraphics[width=\columnwidth]{padding_fail.png}
        \caption{拼接相邻若干帧补全缺省信息的方法失效的例子。左图为经过防抖优化后的原始图像，右图为防抖后补全缺省部分的效果}
        \label{fig:padding_fail}
        \end{center}
    \end{figure}
    
    我认为，可以先将相邻帧的原始图像分割成若干小块，再将每一小块与当前帧做一个匹配和对齐，用小的像素块而非整幅图像去补全缺省信息的方法，可以在一定程度上改善因移动物体而引发的不准确对齐的问题。因为这样的方法在匹配时更注重每一小块的局部位置而非整个图像的全局位置。所以对于移动的物体来说，这样的方法更侧重于对每一小块都拼接出更好的结果。但是这样的做法也有其弊端，一方面是对每一小块进行对齐和匹配必然大大增加了处理时的时间开销；另一方面块大小的选择也是一个难题，每一块太大会使得计算结果对小的移动物体不友好，每一块太小会使得相似的块太多，难以获得较准确的对齐。\\
    
    除了可以采用前后$m$帧的图像进行补全外，还可以先对视频的所有帧计算拼接的全景图，再对每一个输出帧的缺省部分进行补全。使用所有帧的全景图进行拼接的优势在于：第一，效率会有所提高；第二，只采用一定范围内的帧对当前帧进行补全的方法可能会出现不能将视频的空白部分完全补全的情况，所有帧组成的全景图包含了更多的信息。但是，这个方法也有缺陷，其缺陷在于它忽略了视频的时间特性，淡化了时间局部信息。在视频拍摄中，缺省部分的内容有可能是会随着时间的变化而变化的，使用全局全景图的办法难以处理这些随时间变化的缺省内容。 \\
    
    上述的方法在操作上较为简单、方便。如果要追求更好的图片接合效果，可以采用Matsushita Y在Full-frame Video Stabilization\cite{full_frame}中提出的补全视频帧的方法对缺省信息进行补全。
    
    
\section{实验结果}
    我主要使用了老师提供的Dataset中的两个视频进行实验:(1)多人在花园中嬉戏的视频和(2)雪地中不知名动物在奔跑的视频。并设参数为$w_\alpha =10,lb=10,m=15$。 \\
    
    对于第(1)个视频，视频内容主要分为了两个场景。在第一部分，画面基本相似，帧与帧之间有抖动，在这部分基本上可以认为理想画面位置是基本恒定不变的。而在第二部分，画面随着人在地上滚的路径移动，在这一部分，假定理想的相机状态是以恒定的速度向画面的右下方移动的。动态规划计算得到的关键帧为第$1$帧，第$285$帧，第$376$帧，第$389$帧，其中第$1$帧和第$389$帧为起始帧和末尾帧。视频在前285帧相机基本都在同一个场景中，在随后的帧中，蓝黑色衣服的人在地上滚动，相机追随着他的移动。所以该结果与期望基本一致。第$376$帧会被选为关键帧是因为在视频的末尾，相机有停止运动的动作。\\
    
    也可以从$x,y,\alpha$三个分量上的变换来分析最终的关键帧选取结果。从图\ref{fig:DP_motion_c}中可以看出，动态规划的结果能比较好地刻画原始视频的相机路线。由于变换中的旋转分量$\alpha$数值较小，所以在该视频上关键点的选取方案对旋转角度$\alpha$不敏感。但是，使用上述路径规划方法，会使得每两个相邻帧之间都会有微小角度的旋转，在最终结果中，这样的小旋转对观影也是一种干扰。要减轻这样的干扰，我认为有两种可能可行的方法。第一种方法是先对视频进行预处理，先将类似噪声的相机旋转抖动在预处理阶段做一定的抑制。第二种方法考虑到在一般的视频中，使用者只有在进行场景切换路径变换的时候才会希望相机旋转（在此暂时不考虑为了艺术效果的旋转拍摄）。所以可以在规划的线性路径中，取一个平均的角度，设定相机在线性路径上移动时，角度不发生变化。在需要进行线性路径切换的时候，设定一个缓冲区，在缓冲区内相机进行均匀的旋转操作，实现从上一个线性路径的角度到下一个线性路径的角度的变换。但是由于时间的关系，我没有对这样的优化进行实现和实验。\\
    \begin{figure}[!hbt]
      \begin{center}
        \subfigure[$x$分量]{
          \begin{minipage}[b]{0.45\textwidth}
          \includegraphics[width=\textwidth]{DP_motionx_c.jpg}
          \end{minipage}}
        \subfigure[$y$分量]{
          \begin{minipage}[b]{0.45\textwidth}
          \includegraphics[width=\textwidth]{DP_motiony_c.jpg}
          \end{minipage}}
        \subfigure[$\alpha$分量]{
          \begin{minipage}[b]{0.45\textwidth}
          \includegraphics[width=\textwidth]{DP_motionA_c.jpg}
          \end{minipage}}
          \caption{视频(1)相机移动路线与动态规划移动结果(蓝色折线是原始的相机路径，橙色折线是经动态规划优化后的相机路径)}
          \label{fig:DP_motion_c}
      \end{center}
    \end{figure}
    
    对于第(2)个视频，主要分为三个场景。第一个场景中，画面基本保持稳定不变。第二个场景中，抖动幅度较大。第三个场景中，画面追踪“不知名的动物”整个相机向镜头的右下方移动。可以看到图\ref{fig:DP_motion_a}的相机移动路线，直观上看拟合效果还是很不错的。但是，可以看到$y$分量和$\alpha$分量上在第$100-150$帧之间原始相机位置和拟合得到的相机位置有一定的距离，这会导致最终计算出来的输出视频中，真实的当前帧部分只占画面的很小一部分，而剩下的空白部分都需要由相邻的若干帧来补全，由于我最终只实现了用全景图的方法来补全空白部分的缺省信息，所以有一些极端的视频帧处理得相当不好。图\ref{fig:a_fail}就是其中一个极端的情况，由于该部分抖动剧烈，所以为了对齐前后帧，当前帧的信息在取景框中所占比例较小，而位于输出视频上方的人头在前后若干帧中存在着移动，所以也没能很好地对齐。而输出视频中下方及右方大量空白区域的出现主要是因为我在做全景图补全的时候只用了前后的$W(W=15)$帧进行补全，右下方应该出现的大片雪地在$W*2$帧用于补全的视频帧中出现。如果采用视频中所用帧拼接成的全景图对缺省画面进行补全的话，输出视频大片空白部分的问题可以得到解决。\\
    
    \begin{figure}[!hbt]
        \begin{center}
        \includegraphics[width=\columnwidth]{a_fail.png}
        \caption{视频(2)中由于原始视频局部抖动剧烈而处理得不好的第$108$帧}
        \label{fig:a_fail}
        \end{center}
    \end{figure}
    
    \begin{figure}[!hbt]
      \begin{center}
        \subfigure[$x$分量]{
          \begin{minipage}[b]{0.45\textwidth}
          \includegraphics[width=\textwidth]{DP_motionx_a.jpg}
          \end{minipage}}
        \subfigure[$y$分量]{
          \begin{minipage}[b]{0.45\textwidth}
          \includegraphics[width=\textwidth]{DP_motiony_a.jpg}
          \end{minipage}}
        \subfigure[$\alpha$分量]{
          \begin{minipage}[b]{0.45\textwidth}
          \includegraphics[width=\textwidth]{DP_motionA_a.jpg}
          \end{minipage}}
          \caption{视频(2)相机移动路线与动态规划移动结果(蓝色折线是原始的相机路径，橙色折线是经动态规划优化后的相机路径)}
          \label{fig:DP_motion_a}
      \end{center}
    \end{figure}
    
\section{结论与局限}
    用折线拟合相机路线，并采用动态规划来选取关键帧的方法能够获得很好的拟合折线，并且该折线符合人的拍摄习惯。该方法结合用相邻若干帧的图像补全当前输出帧的缺省图像时，能获得较好的效果。但是该方法在存在短时间内相机剧烈抖动的情况下，输出效果会大受影响。\\
    
    另外，由于该方法是基于特征点匹配的，所以在以下几种情况下基本失效。\\
    一、输入视频由于抖动太剧烈，导致每一帧图像有严重的运动模糊现象。在这种情况下，特征点难以正确匹配，直接导致了相机路径无法估计或估计的相机路径严重失真。
    二、输入视频的画面中，大范围的内容是正在移动的物体。这种情况下，大部分特征点都在移动物体上，所以估计的相机路径是以移动物体为基准的，直接导致了估计的相机运动偏离真实值。这样处理出来的视频也是会与观影者的习惯不符的。


\begin{thebibliography}{5}

    \bibitem{GYR1}
Karpenko A, Jacobs D, Baek J, et al. Digital video stabilization and rolling shutter correction using gyroscopes[J]. CSTR, 2011, 1: 2.

    \bibitem{GYR2}
    Hanning G, Forslöw N, Forssén P E, et al. Stabilizing cell phone video using inertial measurement sensors[C]//Computer Vision Workshops (ICCV Workshops), 2011 IEEE International Conference on. IEEE, 2011: 1-8.
    
    \bibitem{L1Opt}
    Grundmann M, Kwatra V, Essa I. Auto-directed video stabilization with robust l1 optimal camera paths[C]//Computer Vision and Pattern Recognition (CVPR), 2011 IEEE Conference on. IEEE, 2011: 225-232.
    
    \bibitem{FLiu1}
    Liu F, Gleicher M, Jin H, et al. Content-preserving warps for 3D video stabilization[C]//ACM Transactions on Graphics (TOG). ACM, 2009, 28(3): 44.
    \bibitem{FLiu2}
    Liu F, Gleicher M, Wang J, et al. Subspace video stabilization[J]. ACM Transactions on Graphics (TOG), 2011, 30(1): 4.
    
    \bibitem{Matlab}
    Video Stabilization Using Point Feature Matching By Matlab.\url{http://cn.mathworks.com/help/vision/examples/video-stabilization-using-point-feature-matching.html}
    
    \bibitem{full_frame}
    Matsushita Y, Ofek E, Tang X, et al. Full-frame video stabilization[C]//Computer Vision and Pattern Recognition, 2005. CVPR 2005. IEEE Computer Society Conference on. IEEE, 2005, 1: 50-57.
    

\end{thebibliography}

% Your document ends here!
\end{document}